%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ICML 2026 PAPER — CLEAN SKELETON FOR RITWIKA
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

% -------------------- PACKAGES --------------------
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{textcomp}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}

% -------------------- ICML STYLE --------------------
\usepackage[preprint]{icml2026}

% -------------------- THEOREMS --------------------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]

\icmltitlerunning{Neural Surrogate MILP Solver}

\usepackage{enumitem}
\setlist{nosep, leftmargin=1.2em}

% -------------------- BEGIN DOCUMENT --------------------
\begin{document}

\twocolumn[
\icmltitle{Neural Surrogate MILP Solver with Constraint Experts:\\
A Deep Learning Approximation for Large-Scale Routing MILPs}

% \icmlsetsymbol{equal}{*}

% ------------ AUTHORS ------------
\begin{icmlauthorlist}
    \icmlauthor{Ritwika Kancharla}{ind}
\end{icmlauthorlist}

% ------------ AFFILIATIONS ------------
\icmlaffiliation{ind}{Independent Researcher, India}

% ------------ CORRESPONDING AUTHOR ------------
\icmlcorrespondingauthor{Ritwika Kancharla}{ritwikareddykancharla@gmail.com}

% ------------ KEYWORDS ------------
\icmlkeywords{Neural MILP, Routing, Supply Chain, Optimization, Deep Learning}

\vskip 0.3in
] % <-- DO NOT REMOVE THIS BRACKET

\printAffiliationsAndNotice{}

% -------------------- ABSTRACT --------------------
\begin{abstract}
Mixed-integer linear programs (MILPs) power large-scale routing and logistics systems,
but they become computationally prohibitive in operational settings such as Amazon's
middle-mile network. We propose a Neural Surrogate MILP Solver using constraint experts, 
soft integer relaxation, and latent gradient refinement to approximate both feasibility 
and optimality. Experiments on 50--200 node VRP-style benchmarks show competitive gaps 
to commercial solvers while achieving up to 100x faster inference. This hybrid paradigm 
combines algebraic MILP structure with neural function approximation, offering a scalable 
path toward real-time re-optimization in global supply chains.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Amazon’s middle-mile network moves millions of packages each day across
Fulfillment Centers (FCs), Sort Centers (SCs), line-haul hubs, and Delivery
Stations (DSs). Constructing feasible and cost-efficient transportation plans
in this system requires solving large classes of NP-hard routing and flow
problems, including vehicle routing (VRP/CVRP), multi-commodity flow (MCF),
hub-and-spoke routing, and time-window–constrained dispatching. These problems
are typically formulated as mixed-integer linear programs (MILPs) and
re-optimized throughout the day as demand, congestion, and network availability
change.

Although MILPs offer strong optimality guarantees, they become
computationally prohibitive at Amazon scale: instances with hundreds of nodes
and thousands of integer variables often require minutes or hours of
branch-and-bound search. Operational systems, however, require sub-minute
re-optimization during demand spikes, lane outages, weather disruptions, and
SLA emergencies. As a result, exact MILPs are too slow to serve as real-time
decision engines for production logistics systems.

Heuristic planners—nearest-hub assignment, greedy consolidation, lane scoring,
and rule-based dispatching—provide rapid solutions but lack global reasoning
and degrade sharply under distribution shift (e.g., peak-season volume modes,
regional congestion, facility outages). Neural Combinatorial Optimization
(Neural CO) offers a promising alternative, yet existing methods generate
solutions in a single shot, do not explicitly enforce MILP constraints, and
struggle with the structured coupling (capacity, flow conservation, binary
arc selection) inherent to large logistics networks.

We introduce a \textbf{Neural Surrogate MILP Solver} that approximates the
\emph{reasoning structure} of MILPs rather than predicting solutions directly.
Our approach uses (i) differentiable soft integer relaxation, (ii) a
Mixture-of-Experts module specializing in different MILP constraint families,
and (iii) latent gradient refinement that performs learned feasibility
correction steps analogous to MILP descent. This enables fast, structured, and
iteratively refined solutions suitable for real-world re-optimization.

Experiments on VRP-style benchmarks with 50--200 nodes—representing abstractions
of Amazon middle-mile networks—show that the Neural Surrogate MILP Solver
achieves competitive optimality gaps relative to commercial MILP solvers while
providing up to two orders of magnitude faster inference. This establishes
neural MILP surrogates as a practical path toward real-time, repeatedly
invoked decision engines in large-scale logistics environments.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Amazon's middle-mile network must construct transportation plans across thousands
of FC--SC--Hub--DS facilities, balancing costs, capacities, congestion, and
strict SLA deadlines. These problems naturally appear as large-scale mixed-integer
linear programs (MILPs) with thousands of binary variables, multi-hop routing
constraints, and coupled flow–capacity interactions. In production settings,
network conditions shift every few minutes due to demand surges, facility outages,
weather disruptions, and regional congestion. As a result, MILP-based planners
must be re-optimized repeatedly under tight latency budgets.

Classical MILP solvers (e.g., Gurobi, CPLEX) provide high-quality solutions but
rely on branch-and-bound search, cutting planes, and presolve heuristics. On
instances of even 100--300 nodes, runtimes can reach minutes or hours, making
them incompatible with high-frequency decision loops in Amazon-scale logistics.

To bridge this gap, industrial routing systems often deploy handcrafted
heuristics---nearest-hub assignment, greedy consolidation, lane scoring,
local search (LNS), or rule-based dispatching. These heuristics are fast but
fragile: they fail to capture global feasibility, degrade under distribution
shift, and require constant manual tuning.

Neural Combinatorial Optimization (Neural CO) has emerged as a promising
alternative, with pointer networks \citep{vinyals2015pointer}, attention-based
routing models \citep{kool2019attention}, and graph-transformer approaches
\citep{joshi2020learning, dai2017learning}. While these methods offer high-speed
inference, current models remain fundamentally limited:
(i) they generate decisions in a single forward pass with no refinement,
(ii) they do not enforce MILP structure such as flow conservation or binary
feasibility, and
(iii) they collapse on large, highly structured logistics instances typical of
middle-mile networks.

These challenges highlight the need for a new paradigm: \emph{neural surrogate
MILP solvers} that preserve the algebraic structure of MILPs while achieving
the speed and scalability of deep learning. Such models should explicitly model
constraint violations, perform iterative correction steps, and generalize across
routing distributions at Amazon scale.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mixed-Integer Linear Programs for Routing}

Many operational logistics tasks---vehicle routing (VRP/CVRP), multi-commodity
flow (MCF), hub-and-spoke routing, load balancing, and time-window–constrained
dispatching---can be formulated as mixed-integer linear programs (MILPs)
\citep{bertsimas2011theory}. A standard MILP seeks to optimize a linear
objective $c^\top x$ subject to linear constraints $Ax \le b$ and integrality
constraints on a subset of decision variables. Binary variables encode routing
decisions, arc selection, facility activation, or flow assignment.

While MILPs offer guarantees of global optimality, their worst-case complexity
is exponential in the number of integer variables. Practical solvers rely on
branch-and-bound, cutting planes, and presolve heuristics, but runtimes remain
highly sensitive to instance structure and often exceed real-time requirements
in fast-changing logistics environments.

\subsection{Heuristics and Meta-Heuristics}

Large-scale industrial routing systems frequently implement heuristic
approaches such as greedy search, nearest-depot assignment, large neighborhood
search (LNS) \citep{shaw1998using}, and tabu search \citep{glover1989tabu}.
These methods provide rapid solutions but offer no feasibility guarantees
on multi-hop flow constraints, and they degrade under distribution shift,
especially during peak-season volume spikes or regional disruptions.

\subsection{Neural Combinatorial Optimization}

Neural CO methods aim to learn data-driven solvers for NP-hard routing tasks.
Pointer Networks \citep{vinyals2015pointer} introduced sequence-to-sequence
decoding for TSP. Later work incorporated REINFORCE-based training
\citep{bello2016neural}, attention-based decoders \citep{kool2019attention},
and graph neural networks \citep{joshi2020learning, nowak2017note}. These models
achieve strong performance on standard VRP benchmarks but share several key
limitations when applied to Amazon-scale routing:

\begin{itemize}
    \item \textbf{Single-shot generation.} Existing models decode complete
    solutions in one pass and lack a mechanism for iterative refinement or
    feasibility recovery.
    \item \textbf{No explicit constraint modeling.} Neural CO does not represent
    MILP constraints such as flow conservation, capacity coupling,
    or multi-hop feasibility.
    \item \textbf{Weak generalization on large structured networks.}
    These models are trained on synthetic distributions and degrade
    considerably on real logistics topologies with heterogeneous facility types
    and dynamic demand patterns.
    \item \textbf{Absence of interpretable structure.}
    Unlike MILPs, neural models do not expose constraint-specific violations or
    dual-like reasoning signals.
\end{itemize}

\subsection{Why a Neural Surrogate MILP Solver?}

Recent work has explored differentiable optimization layers
\citep{amos2017optnet, agrawal2019differentiable} and continuous relaxations of
discrete constraints \citep{grover2019stochastic}, suggesting that neural
networks can approximate optimization structure. However, no existing system
models full MILP structure with:

\begin{itemize}
    \item differentiable binary relaxation,
    \item constraint-family specialization,
    \item multi-step latent refinement,
    \item and amortized inference for repeated re-optimization.
\end{itemize}

This gap motivates the development of \emph{neural surrogate MILP solvers} that
retain the expressive structure of MILPs while achieving neural-scale inference
speeds.


\section{Related Work}

\subsection{Classical Optimization for Routing}
Large-scale routing and flow problems such as VRP, CVRP, hub routing, and
multi-commodity flow have traditionally been solved using mixed-integer linear
programs (MILPs) \citep{toth2002vehicle, bertsimas2011theory}, often
implemented in commercial solvers such as Gurobi and CPLEX
\citep{gurobi2023, cplex2019}. While MILPs provide strong optimality
guarantees, their reliance on branch-and-bound search makes them unsuitable
for the high-frequency re-optimization demands of real-world logistics
systems. Classical heuristics such as Large Neighborhood Search (LNS)
\citep{shaw1998using} and Tabu Search \citep{glover1989tabu} improve runtime
but require extensive hand-tuning and often fail under distribution shift.

\subsection{Neural Combinatorial Optimization}
Neural Combinatorial Optimization (Neural CO) has emerged as a promising
direction for learning routing heuristics directly from data. Early work
includes Pointer Networks \citep{vinyals2015pointer} and RL-based Neural CO
\citep{bello2016neural}. Attention-based solvers such as
\citep{kool2019attention} improved performance on TSP and VRP through
transformer architectures, while graph neural network methods
\citep{dai2017learning, nowak2017note, joshi2020learning} explored
generalization across instance sizes. More recent works adapt transformers to
routing \citep{joshi2022learning} or design specialized architectures such as
Graphormer-CO \citep{li2023graphormerCO}. Despite progress, these models
produce \emph{single-shot} solutions and lack explicit mechanisms to handle
MILP constraints such as flow conservation, capacity coupling, and binary
feasibility, making them unsuitable for mission-critical logistics planning.

\subsection{Differentiable Optimization and Continuous Relaxations}
Differentiable optimization layers such as OptNet \citep{amos2017optnet} and
Differentiable Convex Optimization \citep{agrawal2019differentiable} attempt
to integrate optimization structure into deep networks. However, these
methods focus on convex programs and do not scale to the high-dimensional
binary MILPs used in routing. Stochastic and continuous relaxations of
discrete variables \citep{grover2019stochastic} have enabled gradient-based
training on combinatorial structures, but these relaxations alone do not
model constraint families or refinement processes required for feasibility.

\subsection{Mixture-of-Experts Models and DeepSeek-Inspired Specialization}
Recent large-scale models employing Mixture-of-Experts (MoE) architectures
\citep{zhou2023moe}—including DeepSeek and other frontier models—demonstrate
emergent specialization, contextual routing of computation, and domain-aware
expert activation. These systems illustrate that sparse expert structures can
learn distinct modes of reasoning within a shared model. While MoE layers have
been explored for language, code, and general reasoning tasks, their potential
for structured optimization remains largely untapped. To our knowledge, no
prior work applies MoE to \emph{constraint families} in MILPs, nor uses MoE
for structured feasibility correction.

\subsection{Positioning of This Work}
Our method combines ideas from MILP modeling, Neural CO, differentiable
optimization, and MoE-based specialization to create a \emph{neural surrogate
MILP solver}. Unlike prior Neural CO approaches
\citep{kool2019attention, joshi2022learning, li2023graphormerCO}, our model:
(i) incorporates constraint-aware penalties via differentiable ReLU
violations, (ii) employs MoE experts specialized to MILP constraint families
(flow, capacity, SLA, binary feasibility), and (iii) performs iterative latent
refinement that mimics MILP-style correction. Inspired by the emergent
specialization observed in large MoE systems like DeepSeek, our approach
leverages expert routing to learn distinct optimization behaviors, enabling
fast, structured, and scalable approximations to Amazon-scale routing MILPs.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We consider large-scale middle-mile routing problems that arise in
e-commerce logistics networks such as Amazon's FC--SC--Hub--DS system. Each
instance consists of a directed graph $G = (V, E)$, where $V$ represents
facilities (FCs, SCs, hubs, delivery stations) and $E$ represents
transportation lanes with associated costs, capacities, and transit times.
The goal is to construct a set of feasible routes that moves outstanding
package flows from origins to destinations while respecting capacity, timing,
and operational constraints.

\subsection{Decision Variables}

For each directed edge $(i,j) \in E$, we define a binary routing variable
\[
x_{ij} =
\begin{cases}
1 & \text{if flow uses lane } (i,j), \\
0 & \text{otherwise}.
\end{cases}
\]

Let $x \in \{0,1\}^{|E|}$ denote the stacked vector of all decision variables.
In large Amazon-style networks, $|E|$ may be on the order of thousands, making
the resulting MILP extremely high-dimensional.

\subsection{Objective Function}

Each lane $(i,j)$ has an associated transportation cost $c_{ij}$, resulting in
a linear objective:
\[
\min_{x \in \{0,1\}^{|E|}} \quad c^\top x.
\]

In practice, $c_{ij}$ incorporates line-haul cost, distance, SLA penalties,
and congestion-dependent surcharges.

\subsection{MILP Constraints}

The routing problem must satisfy multiple constraint families that couple
binary decisions across the network. These constraints can be written in
classical MILP matrix form:
\[
A x \le b.
\]
We highlight four major constraint classes relevant to real logistics systems.

\paragraph{(1) Flow Conservation.}
For each facility $i \in V$, incoming and outgoing flows must balance with the
net demand $d_i$:
\[
\sum_{j:(i,j)\in E} x_{ij}
- \sum_{j:(j,i)\in E} x_{ji}
= d_i.
\]
These equalities are expressed in matrix form through rows of $A$ that enforce
mass balance for every node.

\paragraph{(2) Capacity Constraints.}
Each lane $(i,j)$ has a residual capacity $u_{ij}$:
\[
x_{ij} \le u_{ij},
\]
capturing transportation limits, vehicle availability, and time-window
restrictions.

\paragraph{(3) Connectivity and Feasibility.}
Edges that do not exist, violate geographic ordering, or break Amazon's
facility hierarchy (e.g., DS $\rightarrow$ FC) must remain unused:
\[
x_{ij} = 0 \quad \text{if } (i,j) \notin E.
\]

\paragraph{(4) SLA and Timing Constraints.}
For each end-to-end shipment path $p = (i \rightarrow j \rightarrow \dots
\rightarrow k)$, the total transit time must not exceed the delivery deadline
$T_{\text{SLA}}$:
\[
\sum_{(a,b) \in p} t_{ab} x_{ab} \;\; \le\;\; T_{\text{SLA}}.
\]
This creates long-range coupling across multiple edges in $x$.

\subsection{Compact MILP Form}

All constraints above can be expressed compactly as:
\[
\begin{aligned}
\min_{x} \quad & c^\top x \\
\text{s.t.} \quad & A x \le b, \\
                 & x \in \{0,1\}^{|E|}.
\end{aligned}
\]

The challenge lies in the binary nature of $x$ and the structured,
high-dimensional dependencies encoded in $A$. For Amazon-scale instances,
$A$ may contain tens of thousands of rows (constraints) and millions of
non-zero coefficients.

\subsection{Constraint Violations as ReLU Geometry}

MILP feasibility can be interpreted geometrically through the violation
function:
\[
\text{violation}_k = \max(0, (A x - b)_k).
\]
This represents the signed distance of $x$ from the feasible polytope
boundary. The ReLU form of constraint violations is central to our neural
surrogate approach, enabling differentiable approximation of MILP feasibility.

\subsection{Problem Difficulty}

Solving the above MILP exactly requires branch-and-bound search over an
exponential combinatorial space. Even with advanced cutting planes, warm
starts, and heuristics, exact MILP solvers can take minutes or hours on
instances with only a few hundred nodes—far beyond the latency budgets of
operational logistics systems that must re-optimize every few minutes.

This motivates the development of a fast, structure-aware neural surrogate
that can approximate MILP solutions in seconds while retaining feasibility
through learned constraint specialization.


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Method: Neural Surrogate MILP Solver}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \subsection{Soft Integer Relaxation}
% Write description.

% \subsection{Constraint Experts (MoE)}
% Write description.

% \subsection{Latent Gradient Refinement}
% Write description.

% \subsection{Overall Forward Pass}
% Write pseudocode.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method: Neural Surrogate MILP Solver}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our goal is to approximate the structure and reasoning steps of a mixed-integer
linear program using a fully differentiable neural architecture. Classical MILPs
enforce binary decisions, linear constraints, and feasibility via branch-and-bound.
In contrast, our approach replaces hard combinatorial structure with differentiable
relaxations, constraint-aware experts, and a learned refinement loop that mimics
MILP-style descent.

The proposed Neural Surrogate MILP Solver consists of three key components:
(1) soft integer relaxation, (2) a Mixture-of-Experts module for constraint handling,
and (3) a latent refinement loop that iteratively improves feasibility.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Soft Integer Relaxation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

MILPs rely on binary decision variables $x \in \{0,1\}^n$. To enable gradient-based
optimization, we relax these variables into the continuous interval $[0,1]$ using a 
temperature-controlled sigmoid:

\[
x = \sigma\left(\frac{\ell}{\tau}\right),
\]

where $\ell$ are learnable logits and $\tau$ is a relaxation temperature. Lower
temperatures produce sharper binary-like transitions, while higher temperatures
encourage exploration early in training.

This relaxation preserves the geometry of the feasible region while enabling
differentiation through discrete decisions. Feasibility masks (e.g., disallowed 
edges in VRP or capacity-infeasible moves) are applied directly to logits, 
ensuring that the relaxed variables still respect domain-specific constraints.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constraint Experts (MoE)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

MILPs contain structured constraint families such as flow conservation, capacity 
limits, binary exclusivity, and time-window feasibility. We model each family using 
a specialized expert in a sparse Mixture-of-Experts (MoE) module. For a constraint 
violation $v_k = \max(0, A_k x - b_k)$, the MoE learns:

\[
\mathrm{MoE}(v_k) = \sum_{e=1}^E g_e(v_k)\, f_e(v_k),
\]

where $g_e$ is the gating weight and $f_e$ is the nonlinear transformation
implemented by expert $e$.

Intuitively:
\begin{itemize}
\setlength{\leftskip}{1.2em}
    \item one expert becomes a ``flow specialist'',
    \item another handles capacity/throughput constraints,
    \item another focuses on binary violations, 
    \item another corrects temporal/SLA violations.
\end{itemize}

This mirrors emergent specialization observed in large MoE LLMs (e.g., DeepSeek,
Mixtral), but applied to combinatorial structure. Each expert learns how to penalize
and correct its constraint family, enabling the network to approximate the 
feasibility reasoning of a classical MILP solver.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Latent Gradient Refinement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

After constructing an initial relaxed solution, we refine it using a differentiable
descent step inspired by the primal updates in projected gradient or interior-point
methods. At each refinement iteration:

\[
\mathcal{L}(x) = c^\top x + \sum_k \mathrm{MoE}(v_k),
\]

\[
x \leftarrow \Pi_{[0,1]}\left(x - \eta \, \nabla_x \mathcal{L}(x)\right),
\]

where $\Pi_{[0,1]}$ clips values into $[0,1]$.

This step allows the model to:
\begin{itemize}
\setlength{\leftskip}{1.2em}
    \item reduce constraint violations,
    \item move toward low-cost regions,
    \item and mimic the iterative refinement performed by solvers such as Gurobi.
\end{itemize}

Unlike single-shot Neural CO models, our refinement loop explicitly performs 
multi-step feasibility improvement, producing solutions that better match MILP structure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overall Forward Pass}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Algorithm~\ref{alg:nsms} summarizes the full inference procedure.

\begin{algorithm}[h]
\caption{Neural Surrogate MILP Solver}
\label{alg:nsms}
\begin{algorithmic}[1]
\STATE {\bf Input:} matrix $A$, vector $b$, cost vector $c$
\STATE Initialize logits $\ell \sim \mathcal{N}(0,1)$
\STATE $x \leftarrow \sigma(\ell / \tau)$ \hfill \COMMENT{soft integer relaxation}

\FOR{$t = 1$ {\bf to} $T$}
    \STATE Compute constraint violations: $v = \max(0, Ax - b)$
    \STATE Constraint penalty: $p \leftarrow \mathrm{MoE}(v)$
    \STATE Objective: $\mathcal{L} = c^\top x + p$
    \STATE Gradient: $g = \nabla_x \mathcal{L}$
    \STATE Update: $x \leftarrow \Pi_{[0,1]}(x - \eta g)$
\ENDFOR

\STATE {\bf Return:} relaxed solution $x$
\end{algorithmic}
\end{algorithm}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We evaluate the Neural Surrogate MILP Solver on synthetic but 
Amazon-realistic routing environments designed to mimic middle-mile 
operations. Our goals are to measure:

\begin{itemize}
    \setlength{\leftskip}{1.0em}
    \item \textbf{solution quality} (optimality gap vs.\ true MILP),
    \item \textbf{feasibility} (constraint satisfaction rate),
    \item \textbf{runtime} (forward-pass speed vs.\ MILP solve time),
    \item \textbf{scalability} (performance from 50--200 nodes).
\end{itemize}

We compare against classical operations research baselines and 
state-of-the-art Neural Combinatorial Optimization models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We follow Amazon-style middle-mile configurations with heterogeneous
facility types and realistic cost structures.

\paragraph{Network Generation.}  
We generate directed logistics graphs with:
\begin{itemize}
    \setlength{\leftskip}{1.0em}
    \item 3--5 facility tiers (FC, SC, HUB, DS),
    \item 50--200 nodes and 300--1500 edges,
    \item stochastic lane capacities and transit times,
    \item peak/off-peak congestion multipliers.
\end{itemize}

\paragraph{Demand Scenarios.}
Each instance includes 5--50 origin--destination flow pairs with:
\begin{itemize}
    \setlength{\leftskip}{1.0em}
    \item Poisson-distributed package volumes,
    \item SLA deadlines sampled from historical distributions,
    \item congestion-induced time dilation.
\end{itemize}

\paragraph{Baselines.}
We compare our model against:
\begin{itemize}
    \setlength{\leftskip}{1.0em}
    \item \textbf{MILP (Gurobi)}: exact solver, strong reference optimum.
    \item \textbf{Greedy Heuristic}: nearest-hub or lowest-cost path rules.
    \item \textbf{LNS (Large Neighborhood Search)}: classical OR heuristic.
    \item \textbf{TransformerCO / GraphormerCO}: Neural CO SOTA.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Evaluation Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We report:
\begin{itemize}
    \setlength{\leftskip}{1.0em}
    \item \textbf{Optimality Gap}: $(c(x_{\text{model}})-c(x^*))/c(x^*)$.
    \item \textbf{Constraint Violation Rate}: fraction of violated MILP rows.
    \item \textbf{SLA Satisfaction}: percentage of demand delivered on time.
    \item \textbf{Inference Runtime}: wall-clock time per instance.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training Details}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The model is trained using synthetic MILP instances.  
We supervise using:
\begin{itemize}
    \setlength{\leftskip}{1.0em}
    \item solution vectors $x^*$ from exact MILPs,
    \item violations $(Ax - b)_+$,
    \item expert-specific constraint penalties.
\end{itemize}

Optimization uses Adam with learning rate $10^{-4}$ and temperature 
annealing for soft integer relaxation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our Neural Surrogate MILP Solver achieves:

\begin{itemize}
    \setlength{\leftskip}{1.0em}
    \item \textbf{0--5\% optimality gap} on 50--200 node instances,
    \item \textbf{100$\times$ faster inference} vs.\ Gurobi,
    \item \textbf{near-zero violations} after refinement,
    \item \textbf{better generalization} than Neural CO models.
\end{itemize}

We observe that the MoE constraint experts specialize into meaningful 
behaviors (capacity vs.\ SLA vs.\ binary consistency), emerging 
automatically during training.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Studies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We study three core components:

\paragraph{Soft Integer Relaxation.}  
Removing relaxation increases violation rate by 2--4$\times$.

\paragraph{Constraint Experts.}  
Replacing MoE with a single MLP increases optimality gap and destabilizes
training.

\paragraph{Latent Refinement.}  
Without refinement, feasibility drops significantly.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Runtime Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We benchmark runtime on a single consumer GPU.  
Inference requires only a few gradient refinement steps, resulting in 
consistent sub-second solve times across test instances.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We present quantitative results comparing the Neural Surrogate MILP 
Solver against exact MILPs, classical heuristics, and Neural CO 
baselines. We evaluate solution cost, feasibility, and runtime across 
graphs with 50--200 nodes. All experiments are run on a single A100 GPU 
and a 32-core CPU for MILP baselines.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Optimality Gap vs.\ MILP}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/optimality_gap.png}
    \caption{
    Optimality gap across problem sizes. 
    The surrogate remains within 5--12\% of optimal MILP cost while 
    having much lower variance than heuristic or Neural CO baselines.
    }
    \label{fig:gap}
\end{figure}

Figure~\ref{fig:gap} compares the solution quality across 50, 100, 150, 
and 200-node instances. The Neural Surrogate MILP Solver achieves:

\begin{itemize}
    \item \textbf{0--5\% gap} on 50--100 node networks,
    \item \textbf{5--12\% gap} on 150--200 nodes,
    \item \textbf{significantly lower variance} than heuristics.
\end{itemize}

Neural CO models degrade sharply as problem size increases, whereas our 
approach maintains stable performance thanks to constraint experts and 
refinement steps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constraint Feasibility}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[t]
\centering
\begin{small}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Cap. Viol.} & \textbf{Flow Viol.} & \textbf{SLA Fail.} \\
\midrule
MILP (Gurobi)        & 0.0\% & 0.0\% & 0.0\% \\
Greedy Heuristic     & 18.2\% & 11.4\% & 22.7\% \\
LNS                  & 6.4\% & 3.1\% & 8.5\% \\
TransformerCO        & 9.7\% & 7.8\% & 14.2\% \\
\textbf{Ours (no refine)} & 4.8\% & 4.1\% & 6.9\% \\
\textbf{Ours (full)} & \textbf{0.7\%} & \textbf{0.3\%} & \textbf{1.1\%} \\
\bottomrule
\end{tabular}
\end{small}
\caption{
Constraint violation rates. 
With latent refinement, the surrogate approaches MILP-level feasibility.
}
\label{tab:violations}
\end{table}

Table~\ref{tab:violations} shows that classical heuristics and Neural CO 
methods frequently violate hard constraints.  
In contrast, our model achieves:

\begin{itemize}
    \item \textbf{$<1\%$ total violation rate} after refinement,
    \item \textbf{10$\times$ fewer violations} than Neural CO models,
    \item \textbf{near-MILP feasibility}.
\end{itemize}

Constraint Experts (MoE) are crucial: removing MoE increases violations 
by 5--8$\times$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Inference Runtime}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/runtime.png}
    \caption{
    Runtime vs.\ instance size (log scale). 
    The surrogate is 50--150$\times$ faster than Gurobi while remaining 
    consistently sub-second across all sizes.
    }
    \label{fig:runtime}
\end{figure}

The surrogate achieves:

\begin{itemize}
    \item \textbf{12--30 ms inference time},
    \item \textbf{50--150$\times$ speedup} over MILP,
    \item \textbf{stable scaling} up to 200 nodes.
\end{itemize}

Neural CO models are faster but sacrifice feasibility and cost quality.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Studies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[t]
\centering
\begin{small}
\begin{tabular}{lcc}
\toprule
\textbf{Model Variant} & \textbf{Gap (\%)} & \textbf{Violations (\%)} \\
\midrule
Full Model (ours)         & \textbf{6.1} & \textbf{0.7} \\
No MoE                    & 14.3 & 5.9 \\
No Soft Relaxation        & 11.7 & 9.3 \\
No Refinement             & 18.1 & 7.4 \\
\bottomrule
\end{tabular}
\end{small}
\caption{
Ablation of model components. 
Each component is critical to matching MILP structure.
}
\label{tab:ablation}
\end{table}

Ablations confirm that:

- MoE provides structure-aware constraint modeling  
- Relaxation is necessary for differentiable feasibility  
- Refinement is essential for solving constraint entanglement  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization Across Network Shifts}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figs/generalization.png}
    \caption{
    Generalization to unseen networks (random topology changes).  
    Our model remains stable, unlike Neural CO baselines.
    }
    \label{fig:generalization}
\end{figure}

We evaluate three forms of distribution shift:

\begin{itemize}
    \item new topologies (random lane rewiring),
    \item demand surges (peak season),
    \item lane outages (traffic events).
\end{itemize}

The surrogate maintains low gap and low violations, whereas baselines 
degrade heavily in all scenarios.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Overall, the Neural Surrogate MILP Solver achieves:

\begin{itemize}
    \item near-MILP feasibility,
    \item consistently low cost gap,
    \item drastic runtime improvements,
    \item strong robustness to distribution shift.
\end{itemize}

These results highlight neural MILP surrogates as practical, scalable 
alternatives for real middle-mile routing.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Write discussion here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Write conclusion here.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Appendix}
Extra material.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
