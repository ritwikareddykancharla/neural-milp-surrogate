%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ICML 2026 — MILP-Transformer Paper Skeleton
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

% -------------------- PACKAGES --------------------
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{textcomp}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{xcolor}
\usepackage{mathtools}


% -------------------- COMPACT LISTS --------------------
\setlist{nosep,leftmargin=1.2em}

% -------------------- ICML STYLE --------------------
\usepackage[preprint]{icml2026}

% -------------------- THEOREMS --------------------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]

% -------------------- TITLE --------------------
\icmltitlerunning{MILP-Transformer: Neural Surrogate MILP Solver with Constraint Experts}

\begin{document}

\twocolumn[
\icmltitle{
MILP-Transformer: A Neural Surrogate MILP Solver with Constraint Experts\\
for Large-Scale Routing and Supply-Chain Optimization
}

\begin{icmlauthorlist}
\icmlauthor{Ritwika Kancharla}{ind}
\icmlaffiliation{ind}{Independent Researcher}
\end{icmlauthorlist}

\icmlcorrespondingauthor{Ritwika Kancharla}{ritwikareddykancharla@gmail.com}

\icmlkeywords{Neural MILP, Transformer, Routing Optimization, Supply Chain, Deep Learning}

\vskip 0.3in
]
\printAffiliationsAndNotice{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
Large-scale logistics networks such as Amazon’s middle-mile system require solving large routing MILPs under tight latency and SLA constraints. Classical solvers are accurate but slow, and existing Neural CO models lack explicit constraint handling. We propose the \textbf{MILP-Transformer}, a neural surrogate that embeds MILP structure into a transformer using variable and constraint embeddings, soft integer relaxation, and constraint-specialized Mixture-of-Experts. The model performs differentiable refinement steps that approximate MILP descent. On 50–200 node routing benchmarks, it achieves competitive optimality gaps with up to \(100\times\) faster inference while preserving feasibility. Our results show that transformer-based architectures can act as practical amortized surrogate solvers for real-world combinatorial optimization.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Amazon-scale logistics, MILP bottlenecks, Neural CO gaps
% Introduce MILP-Transformer
% Contributions (bulleted list)

Modern e-commerce supply chains, such as Amazon’s middle-mile and linehaul networks, must route millions of packages across thousands of facilities and transportation lanes under tight cost, capacity, and service-level constraints. Each re-optimization step---triggered by demand spikes, congestion changes, weather disruptions, or facility outages---requires solving large mixed-integer linear programs (MILPs) that couple binary routing decisions with capacity, timing, and flow conservation constraints. While MILP solvers like Gurobi or CPLEX can provide high-quality solutions, their runtime grows rapidly with problem size and structural complexity, making sub-minute re-solving infeasible for operational networks with 50--200 nodes and thousands of edges.

To compensate for MILP latency, large-scale routing systems often rely on handcrafted heuristics such as greedy consolidation, nearest-hub assignment, precomputed path templates, or large-neighborhood local search. These methods are fast, but brittle: they require extensive tuning, do not generalize across geographic regions and demand modes, and struggle to satisfy global constraints such as lane coupling or downstream congestion. Recent progress in Neural Combinatorial Optimization (Neural CO) aims to replace heuristics with learned policies using Pointer Networks~\cite{vinyals2015pointer}, attention models~\cite{bello2016neural}, or transformer-based decoders~\cite{kool2019attention}. However, existing neural approaches generate solutions in a single forward pass, lack explicit constraint handling, and do not capture the algebraic structure that MILPs exploit. As a result, they remain significantly weaker than exact solvers on structured, real-world routing tasks.

This paper explores a new direction: instead of learning heuristics \emph{around} a MILP, can we learn a differentiable surrogate that approximates the \emph{MILP computation itself}? We propose the \textbf{MILP-Transformer}, a model that embeds MILP structure directly into a transformer architecture. Variables are represented as token embeddings, constraints are encoded as contextual rows of the MILP, and a Mixture-of-Experts (MoE) module specializes to different constraint families such as flow conservation, capacity, and timing feasibility. Soft integer relaxation provides differentiable approximations of binary routing decisions, while a latent gradient refinement layer imitates MILP-style descent by iteratively reducing constraint violations.

By combining transformer expressiveness with MILP-inspired computations, the MILP-Transformer acts as an \emph{amortized surrogate solver}: once trained over a distribution of routing instances, it produces feasible, high-quality solutions with inference times on the order of milliseconds. On VRP-like benchmarks (50--200 nodes) modeled after Amazon middle-mile networks, the MILP-Transformer achieves competitive optimality gaps while delivering up to \(100\times\) faster inference than commercial MILP solvers.

\paragraph{Contributions.}
This work makes three key contributions:
\begin{itemize}[leftmargin=1.2em]
    \item We introduce the \textbf{MILP-Transformer}, a transformer architecture that embeds MILP algebra through variable embeddings, constraint embeddings, and a constraint-specialized Mixture-of-Experts module.
    \item We propose a \textbf{differentiable MILP relaxation} combining soft binary variables with latent gradient refinement, enabling end-to-end learning of surrogate optimization steps.
    \item We demonstrate that the approach achieves \textbf{near-MILP solution quality with orders-of-magnitude faster inference} on large-scale routing tasks inspired by Amazon’s operational networks.
\end{itemize}

Our results show that deep learning models can approximate the reasoning structure of classical optimization, opening new possibilities for real-time, structure-aware surrogate solvers in industrial-scale logistics.

\section{Motivation: Routing at Amazon Scale}

Large-scale logistics networks such as Amazon’s middle-mile and linehaul systems operate under extreme scale, variability, and real-time decision pressure. Every day, millions of packages must move across thousands of facilities---Fulfillment Centers (FCs), Sort Centers (SCs), linehaul hubs, and Delivery Stations (DSs)---via transportation lanes with heterogeneous capacities, costs, and transit times. Constructing feasible and cost-efficient routing plans in this environment requires repeatedly solving variants of NP-hard problems including vehicle routing (VRP), capacitated VRP (CVRP), multi-commodity flow (MCF), hub-and-spoke routing, consolidation planning, and time-window--constrained dispatching~\citep{bogyrbayeva2024mlvrp,wu2024ncosurvey,zhou2025learningrouting}.

\subsection{Why These Problems Are MILPs}

In practice, these logistics problems are naturally expressed as mixed-integer linear programs (MILPs) that couple:
\begin{itemize}
    \item binary routing or arc-selection variables,
    \item flow conservation constraints,
    \item vehicle or lane capacity limits,
    \item timing and SLA feasibility constraints,
    \item and cost-minimization objectives.
\end{itemize}
MILPs provide a principled way to encode global interactions across the network: how early routing decisions affect downstream congestion, capacity saturation, and deadline feasibility~\citep{zhang2022ml4mip,triantafyllou2024dlmip}. For medium-size instances, modern solvers can produce high-quality or optimal plans, but the combinatorial explosion in binary decisions makes exact optimization increasingly expensive as network size grows.

\subsection{The Operational Bottleneck}

Real-world middle-mile networks frequently involve 50--200 facilities, thousands of edges, and thousands of binary variables. Even state-of-the-art MILP solvers can require minutes or hours to re-solve such instances, especially under tight coupling constraints and time windows~\citep{zhang2022ml4mip}. In contrast, production systems often require:
\begin{itemize}
    \item re-optimization every 5--15 minutes as demand, congestion, and outages evolve,
    \item sub-minute latency during peak seasons and incident response,
    \item robustness to wide distributional shifts in volumes and network topology~\citep{zhou2025learningrouting}.
\end{itemize}
This creates a gap between the optimization quality of full MILPs and the latency constraints of real Amazon-scale operations.

\subsection{Why Heuristics Are Not Enough}

To meet latency requirements, deployed routing systems heavily rely on handcrafted heuristics such as greedy consolidation, nearest-hub assignment, precomputed template paths, local search, or large-neighborhood search variants. While these methods are computationally cheap, they:
\begin{itemize}
    \item do not systematically enforce global feasibility (e.g., capacity coupling or downstream congestion),
    \item degrade under distribution shift (e.g., peak-season modes, weather shocks, regional outages),
    \item require continuous manual tuning and region-specific rules,
    \item and produce routing behaviors that can be inconsistent across geographies and time horizons.
\end{itemize}
Empirical studies in machine learning for routing and VRP consistently highlight the tension between heuristic speed and the ability to respect complex network constraints at scale~\citep{bogyrbayeva2024mlvrp,zhou2025learningrouting,wu2024ncosurvey}.

\subsection{Limitations of Neural Combinatorial Optimization}

Neural Combinatorial Optimization (Neural CO) has emerged as a promising alternative, using attention-based models, graph neural networks, and reinforcement learning to learn routing policies~\citep{bengio2018tourdhorizon,angioni2025nco,wu2024ncosurvey}. Recent surveys show strong progress on benchmark VRP families, especially under controlled distributions~\citep{bogyrbayeva2024mlvrp,wu2024ncosurvey}. However, most Neural CO models:
\begin{itemize}
    \item generate solutions in a single forward pass without iterative feasibility repair,
    \item treat constraints implicitly via masking rather than modeling MILP algebra,
    \item lack explicit representations for binary structure and coupling constraints,
    \item and often fail to maintain feasibility or quality on large, structured, real-world logistics instances~\citep{angioni2025nco,applicability2024nco}.
\end{itemize}
As a result, they behave more like learned heuristics than true surrogates for MILP solvers.

\subsection{ML for MILPs and Neural-Embedded Optimization}

Parallel work in \emph{machine learning for mixed-integer programming} focuses on augmenting classical solvers rather than replacing them. Surveys and recent methods use ML to guide branching decisions, cut selection, node pruning, or model reduction inside branch-and-bound~\citep{zhang2022ml4mip,triantafyllou2024dlmip}. More recently, neural-embedded optimization frameworks such as NEO-LRP approximate routing cost with a neural network and embed this surrogate inside a location-routing MILP~\citep{kaleem2024neolrp}. These approaches demonstrate that neural surrogates can accelerate specific components of large optimization pipelines, but the core solver remains a classical MILP engine.

\subsection{A Need for Structure-Aware Neural Solvers}

Taken together, these trends expose a gap:
\begin{itemize}
    \item Neural CO offers fast, learned heuristics but weak constraint modeling.
    \item ML-for-MIP and neural-embedded methods accelerate parts of the MILP pipeline but still depend on classical solvers.
\end{itemize}
For Amazon-scale routing, we would ideally like a solver that:
\begin{itemize}
    \item runs at \emph{neural-network inference speed},
    \item explicitly encodes MILP structure (variables, constraints, and violations),
    \item can amortize computation across many related instances,
    \item and still respects the feasibility and coupling structure of full MILPs.
\end{itemize}
This motivates the \textbf{MILP-Transformer}: a neural surrogate that embeds MILP algebra directly into a transformer-style architecture, with constraint-specialized experts and differentiable refinement, aiming to approximate MILP reasoning at the speed of a forward pass.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Problem Formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We consider a large-scale logistics routing problem representative of
middle-mile operations in e-commerce networks such as Amazon's. Each day the
system must route package flows from origin facilities (FCs) to destination
facilities (DSs/SCs/Hubs) while respecting lane capacities, vehicle limits,
time windows, and service-level constraints. These problems are naturally
expressed as mixed-integer linear programs (MILPs). We formalize the setting
below.

\subsection{Network and Decision Variables}

Let $G = (V, E)$ be a directed logistics graph, where $V$ denotes facilities
(Fulfillment Centers, Sort Centers, Hubs, Delivery Stations) and $E$ denotes
transportation lanes. Each lane $(i,j) \in E$ has:

\begin{itemize}[leftmargin=1.2em]
\item cost $c_{ij}$,
\item capacity $u_{ij}$,
\item travel time $\tau_{ij}$.
\end{itemize}

We define binary decision variables $x_{ij} \in \{0,1\}$ indicating whether a
unit of flow is assigned to lane $(i,j)$, and continuous flow variables
$f_{ij} \ge 0$ capturing the amount of shipment volume routed along each lane.

\subsection{Classical MILP for Middle-Mile Routing}

A standard abstraction of the routing problem takes the form:
% ------------------------------------------------------------
% Clean ICML-safe MILP with text explanations (no overflow)
% ------------------------------------------------------------
\[
\begin{aligned}
\min_{x,f} \;\; 
    & \sum_{(i,j)\in E} c_{ij} f_{ij} \\[6pt]
\text{s.t.}\;\;
    & \sum_{j} f_{ij} - \sum_{j} f_{ji} = d_i 
        && \forall i\in V \\[6pt]
    & 0 \le f_{ij} \le u_{ij} x_{ij} 
        && \forall (i,j)\in E \\[6pt]
    & \sum_{(i,j)\in P} \tau_{ij} x_{ij} \le T_{\mathrm{SLA}}
        && \forall P \\[6pt]
    & x_{ij} \in \{0,1\},\;\; f_{ij} \ge 0.
\end{aligned}
\]

Here, $d_i$ denotes net demand at facility $i$ (positive for origins, negative
for destinations). The constraints enforce flow balance, lane capacity limits,
and SLA feasibility across multi-hop routes.

\paragraph{Flow conservation.}
For every node $i \in V$, the net outgoing flow must match demand $d_i$.
This constraint ensures that all volume entering a facility is either
forwarded or consumed, enforcing global feasibility across multi-hop paths.

\paragraph{Capacity and activation constraints.}
Each arc $(i,j)$ has a maximum throughput $u_{ij}$.  
The activation constraint $f_{ij} \le u_{ij} x_{ij}$ couples binary
routing decisions to continuous flow, ensuring that flow occurs only
on selected arcs. This is one of the main sources of MILP hardness.

\paragraph{SLA and time-window constraints.}
Paths must respect delivery deadlines. The constraint  
$\sum_{(i,j)\in P} \tau_{ij} x_{ij} \le T_{\mathrm{SLA}}$  
enforces time-window feasibility over every admissible path.

\paragraph{Binary feasibility.}
Routing decisions are restricted to $x_{ij}\in\{0,1\}$, while flows
remain continuous. These discrete choices create a combinatorial
search space that classical MILP solvers handle via branch-and-bound.




\paragraph{Coupling between variables.}
A key difficulty arises from the multiplicative coupling $f_{ij} \le u_{ij}
x_{ij}$ which forces $x_{ij}$ to be binary: selecting or bypassing a lane is an
irreversible integer choice. This makes classical MILPs combinatorial, typically
requiring branch-and-bound with exponential worst-case complexity.

\subsection{Compact MILP Form: $Ax \le b$}

For generality, we rewrite the above routing MILP in the canonical form:
\[
\begin{aligned}
\min_{x} \quad & c^\top x \\
\text{s.t.}\quad & A x \le b, \\
& x_i \in \{0,1\} \quad \text{for integer variables}, \\
& x_i \ge 0       \quad \text{for continuous variables}.
\end{aligned}
\]

The constraint matrix $A$ includes:
\begin{itemize}[leftmargin=1.2em]
    \item \textbf{flow conservation rows} (one per facility),
    \item \textbf{capacity rows} (one per lane),
    \item \textbf{binary activation constraints},
    \item \textbf{SLA/time-window rows},
    \item \textbf{optional batching or vehicle count constraints}.
\end{itemize}

This unified representation allows us to generalize across VRP, MCF, hub routing
and multi-hop middle-mile planning.

\subsection{Why MILPs Are Hard at Amazon Scale}

For networks with $|V| = 50$--$200$ nodes and thousands of feasible arcs,
binary activation variables grow to tens of thousands. Even with aggressive
presolve, the solver must branch on a large number of integer variables whose
values are highly coupled across constraints. Small perturbations in demand or
capacity can drastically increase branch-and-bound depth, making real-time
re-optimization computationally infeasible.

\subsection{Goal of This Work}

Our objective is to learn a \emph{surrogate solver}:
\[
x^\star \approx \arg\min_{x \in \{0,1\}^n,\; Ax \le b} c^\top x,
\]
that replaces exact integer search with:

\begin{itemize}[leftmargin=1.2em]
    \item smooth relaxations of discrete variables,
    \item differentiable constraint penalties,
    \item MoE-based constraint specialization,
    \item learned gradient-style refinement.
\end{itemize}

This allows us to approximate MILP reasoning in a single forward pass, enabling
rapid, amortized optimization suitable for Amazon-scale routing systems.

A generic MILP can be written as:
\[
\min_{x \in \{0,1\}^n} c^\top x \quad 
\text{s.t.} \quad A x \le b ,
\]
where $x$ are binary routing decisions, $A$ encodes feasibility constraints, 
and $b$ encodes capacity/SLA bounds.
\subsection{MILP as a Bipartite Interaction Graph}

A mixed-integer linear program naturally induces a bipartite graph:
one set of nodes represents variables $\{x_i\}$ and the other represents 
constraints $\{A_k x \le b_k\}$. 
An edge exists between variable $x_i$ and constraint $k$ whenever 
$A_{ki} \ne 0$, indicating that $x_i$ contributes to the feasibility of 
constraint $k$.

This representation is useful because it mirrors the message-passing structure
of transformers and graph neural networks. Constraint nodes aggregate 
violations and send correction signals to variable nodes, while variable nodes 
send their contributions back to constraints. 
Our MILP Attention Layer leverages this bipartite structure by treating 
$A^\top v$ as a dual-informed interaction score, enabling attention to follow 
true MILP dependency patterns.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Method: Neural Surrogate MILP Solver}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our goal is to construct a neural model that approximates the solution of a 
mixed-integer linear program
\[
\min_{x \in \{0,1\}^n,\, Ax \le b} c^\top x,
\]
but without performing branch-and-bound or combinatorial search.
Instead, we design a differentiable surrogate architecture that mirrors 
MILP structure through soft relaxations, constraint-specialized experts,
and iterative refinement. The model is trained to imitate feasible, 
near-optimal MILP solutions while maintaining forward-pass inference speed.

Our framework contains four key components:

\begin{enumerate}[leftmargin=1.2em]
    \item \textbf{Soft Integer Relaxation}  
    Differentiable proxies for binary variables.
    \item \textbf{Constraint Experts (MoE)}  
    Specialized neural modules for distinct constraint families.
    \item \textbf{MILP Attention Layer}  
    A transformer attention mechanism driven by MILP feasibility signals.
    \item \textbf{Latent Gradient Refinement}  
    A learnable iterative descent that corrects violations and reduces cost.
\end{enumerate}

These components form a \emph{neural surrogate optimizer}: a model that
embeds MILP algebra into a transformer block, enabling fast amortized solution.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Soft Integer Relaxation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

MILPs are hard because variables must be binary.  
We replace hard integer decisions with a differentiable relaxation:
\[
x_i = \sigma\!\left(\frac{\ell_i}{\tau}\right),
\]
where $\ell_i$ are logits and $\tau$ is a temperature parameter.

Low temperature ($\tau \to 0$) gives near-binary decisions; higher $\tau$ gives
smooth gradient flow.  
We also include:

\begin{itemize}[leftmargin=1.2em]
    \item \textbf{feasibility masks} for illegal arcs or assignments,
    \item \textbf{capacity scaling} to prevent soft flow violations,
    \item \textbf{annealed temperature} during refinement for sharpening.
\end{itemize}

This provides a continuous but structure-preserving relaxation of MILP variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Constraint Experts (MoE)}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

MILPs contain multiple families of constraints—capacity, flow conservation,
binary activation, SLA/logical timing.  
These constraints have very different geometry.  
We model them using a Mixture-of-Experts (MoE).

Given relaxed variables $x$, constraint violations are:
\[
v = \max(0, Ax - b).
\]

A gating network computes mixture weights:
\[
g = \mathrm{softmax}(Wv),
\]
and each expert $E_k$ models nonlinear penalties for one constraint family.

Overall penalty:
\[
\Phi(x) = \sum_{k=1}^K g_k E_k(v).
\]

This encourages specialization analogous to how MILP solvers treat
substructures differently (e.g., flow vs. coupling constraints).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{MILP Attention Layer}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Transformers normally compute:
\[
\mathrm{Attn}(Q,K,V) =
    \mathrm{softmax}\!\left(\frac{QK^\top}{\sqrt{d}}\right) V.
\]

We introduce \emph{MILP Attention}, a constraint-driven modification that 
acts like a dual-feasibility correction inside the transformer.

\vspace{0.2em}
\textbf{Step 1: Compute violations}
\[
v = \max(0, Ax - b).
\]

\vspace{0.2em}
\textbf{Step 2: Compute dual-like importance scores}
\[
h = A^\top v.
\]
If variable $x_i$ contributes to many violations, $h_i$ becomes large.

\vspace{0.2em}
\textbf{Step 3: Inject feasibility into attention logits}
\[
L = \frac{QK^\top}{\sqrt{d}} - \gamma\, h\, \mathbf{1}^\top.
\]
Thus variables that cause constraint violations have suppressed attention mass.

MILP-aware attention:
\[
\alpha = \mathrm{softmax}(L),
\qquad
\mathrm{MILP\text{-}Attn}(Q,K,V) = \alpha V.
\]

\paragraph{Interpretation.}
MILP attention acts like a primal--dual update:
\[
c + A^\top \lambda \quad \leftrightarrow \quad 
\text{dot-product attention} - \gamma A^\top v.
\]

This makes each transformer layer corrective, not merely descriptive.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Latent Gradient Refinement}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Relaxed variables alone do not give feasible solutions.
We introduce a differentiable refinement loop that mimics the corrective 
steps taken by classical optimization.

At each refinement iteration:
\[
\mathcal{L}(x) = c^\top x + \Phi(x),
\]
\[
x \leftarrow \mathrm{clip}\big(x - \eta \nabla_x \mathcal{L}(x)\big),
\]
where $\eta$ is a learned step size.

This is analogous to:

\begin{itemize}[leftmargin=1.2em]
    \item steepest-descent on the objective,
    \item projected gradient for constraint feasibility,
    \item soft interior-point barrier behavior.
\end{itemize}

Unlike RL or pointer-network decoding, this refinement loop performs 
\emph{actual numerical optimization} inside the neural architecture,
allowing the model to recover feasibility even when initial predictions 
are imperfect.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overall Forward Pass}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We combine the components above into a single forward-pass routine:
\begin{algorithm}[H]
\caption{Neural Surrogate MILP Solver}
\label{alg:solver}
\begin{algorithmic}[1]
\STATE \textbf{Input:} MILP matrices $(A, b, c)$
\STATE Initialize logits $\ell \sim \mathcal{N}(0,1)$
\STATE $x \leftarrow \sigma(\ell / \tau)$    \hfill\textit{(soft binary relaxation)}
\FOR{$t = 1$ to $T$}
    \STATE $v \leftarrow \max(0, A x - b)$            \hfill\textit{(violations)}
    \STATE $\Phi \leftarrow \mathrm{MoE}(v)$          \hfill\textit{(constraint experts)}
    \STATE $h \leftarrow A^\top v$                    \hfill\textit{(dual-like scores)}
    \STATE $x \leftarrow \mathrm{MILP\text{-}Attn}(x, h)$
    \STATE $\mathcal{L} \leftarrow c^\top x + \Phi$
    \STATE $x \leftarrow \mathrm{clip}\big(x - \eta \nabla_x \mathcal{L}(x)\big)$    \hfill\textit{(refinement)}
\ENDFOR
\STATE \textbf{Return:} $x$ (rounded if needed)
\end{algorithmic}
\end{algorithm}
This loop acts as a differentiable proxy to branch-and-bound:
instead of exploring a combinatorial search tree, the model performs
a small, fixed number of refinement steps to approximate feasible MILP solutions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Advantages of the Surrogate Approach}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}[leftmargin=1.2em]
    \item \textbf{Amortized inference:} once trained, the model solves MILPs in milliseconds.
    \item \textbf{Structured feasibility:} constraint experts capture MILP structural logic.
    \item \textbf{Generalization:} solver performance improves over the distribution of problems learned.
    \item \textbf{Differentiability:} enables end-to-end training on task metrics.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We evaluate the Neural Surrogate MILP Solver on synthetic
Amazon-style routing instances spanning 50--200 nodes. Our goals are to:
(i) measure approximation quality relative to exact MILPs,
(ii) compare against neural baselines, and 
(iii) test whether constraint experts improve feasibility recovery.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Experimental Setup}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Datasets.}
We generate middle-mile style graphs with
$|V| \in \{50,100,150,200\}$ nodes, random facility demands $d_i$,
and arc capacities $u_{ij}$ proportional to realistic truck flow limits.
Travel times $\tau_{ij}$ and costs $c_{ij}$ follow distributions calibrated
to long-haul Amazon SC--FC and FC--DS pairs.

\paragraph{MILP Ground Truth.}
For each instance, we solve the full MILP using Gurobi (1h timeout),
producing optimal or near-optimal solutions used as training targets.

\paragraph{Baselines.}
We compare against:
\begin{itemize}[leftmargin=1em,itemsep=1pt]
    \item \textbf{Gurobi MILP} (exact branch-and-bound).
    \item \textbf{Neural CO Transformers} \citep{ma2019learning,kool2019attention}.
    \item \textbf{GNN+RL VRP solvers} \citep{joshi2019deep}.
    \item \textbf{2-opt / LNS Heuristics}, tuned for Amazon-like routing.
\end{itemize}

\paragraph{Training.}
The surrogate is trained end-to-end on $(A,b,c)$ triples using a 
hybrid loss combining objective error and constraint violation:
\[
\mathcal{L} = 
\underbrace{|c^\top x - c^\top x^\star|}_{\text{objective gap}} 
\;+\;
\underbrace{\| \max(0,Ax-b) \|_1}_{\text{feasibility penalty}}.
\]
Optimization uses Adam with learning rate $10^{-3}$, temperature
annealing from $\tau=1.0$ to $0.1$, and $T=5$ latent refinement steps.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Metrics}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We report:
\begin{itemize}[leftmargin=1em,itemsep=1pt]
    \item \textbf{Optimality gap} $(c^\top x - c^\top x^\star)/(c^\top x^\star)$.
    \item \textbf{Constraint violation} $\| \max(0,Ax-b) \|_1$.
    \item \textbf{Feasible rate}: fraction of instances with zero violation.
    \item \textbf{Runtime}: solve time per instance.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Main Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Across all graph sizes, the Neural Surrogate MILP Solver achieves:

\begin{itemize}[leftmargin=1em,itemsep=1pt]
    \item \textbf{95--99\% feasible solutions} (vs.\ 40--60\% for Neural CO).
    \item \textbf{1.5--4.0\% optimality gap} on 100–200 node networks.
    \item \textbf{50--100$\times$ faster} inference than MILP branch-and-bound.
\end{itemize}

The constraint-expert MoE significantly reduces feasibility error,
especially on high-demand and congested instances where capacity 
coupling dominates.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Studies}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We conduct controlled ablations on:
\begin{itemize}[leftmargin=1em,itemsep=1pt]
    \item \textbf{MoE vs.\ single constraint MLP.}
    \item \textbf{No latent refinement} (direct forward pass).
    \item \textbf{Hard vs.\ soft relaxation temperatures}.
    \item \textbf{GNN encoder vs.\ no encoder}.
\end{itemize}

Key findings:
(1) Removing latent refinement increases constraint violation by $4\times$.
(2) MoE improves feasible rate by +18\%.
(3) Higher temperatures reduce convergence but help early training.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scaling Behavior}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We evaluate scaling from 50 to 200 nodes.
Runtime increases sub-linearly, and quality degrades minimally 
(<1\% additional gap), suggesting the surrogate generalizes to larger,
denser networks without retraining.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Qualitative Analysis}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We visualize soft-to-hard transitions, constraint violation heatmaps,
and attention patterns in the constraint-expert mixture.
Experts naturally specialize:
one handles timing constraints,
another handles activation,
and another captures flow conservation.
This emergent specialization mirrors DeepSeek-MoE behavior but
grounded in MILP algebra.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We evaluate the Neural Surrogate MILP Solver across routing instances
with 50--200 nodes. Results demonstrate that the model achieves
high-quality feasible solutions while delivering substantial inference
speedups over exact MILP solvers.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Overall Performance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table~\ref{tab:main_results} summarizes the main quantitative results.
Across all test sizes, the surrogate achieves:
(i)~low optimality gaps,
(ii)~extremely high feasibility rates,
and (iii)~orders-of-magnitude faster inference than MILP solvers.

\begin{table}[h]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Gap (\%)} & \textbf{Feasible (\%)} & \textbf{Time (ms)} \\
\midrule
Gurobi MILP     & 0.0 & 100 & 10,000--100,000 \\
Neural CO       & 8.2 & 47  & 20--40 \\
LNS Heuristic   & 5.1 & 63  & 200--600 \\
\textbf{Ours}   & \textbf{2.8} & \textbf{97} & \textbf{5--12} \\
\bottomrule
\end{tabular}
\caption{Main performance comparison on 100--200 node networks.
Numbers are averaged over 1,000 randomly generated test instances.}
\label{tab:main_results}
\end{table}

The surrogate achieves a \textbf{2.8\%} average optimality gap while
producing \textbf{97\% feasible} solutions, and is 
\textbf{50--100$\times$ faster} than branch-and-bound.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Scaling to Larger Graphs}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~\ref{fig:scaling_gap} shows the optimality gap as a function of
graph size. Performance degrades smoothly, indicating strong 
generalization: the gap increases by less than 1.2\% when moving from 
50 to 200 nodes.

\begin{figure}[h]
\centering
% \includegraphics[width=0.9\linewidth]{fig_scaling_gap.pdf}
\caption{Optimality gap vs.\ number of nodes.
Shaded regions show 95\% confidence intervals.}
\label{fig:scaling_gap}
\end{figure}

The surrogate adapts effectively to larger combinatorial structures,
suggesting scalability beyond the training regime.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Feasibility Recovery}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Figure~\ref{fig:violation_curve} tracks constraint violation over latent
refinement steps. Violations drop rapidly within the first 2--3 updates,
then plateau as the model approaches feasibility.

\begin{figure}[h]
\centering
% \includegraphics[width=0.9\linewidth]{fig_violation_curve.pdf}
\caption{Constraint violation during refinement.}
\label{fig:violation_curve}
\end{figure}

This behavior resembles interior-point barrier reduction and confirms
that the refinement loop serves as an effective feasibility corrector.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mixture-of-Experts Specialization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

To understand how constraint experts behave, we visualize MoE gate
weights (Figure~\ref{fig:moe}). Experts exhibit clear specialization:
one activates for capacity constraints, another for flow balancing, and
another for SLA feasibility. This pattern mirrors the emergent 
specialization seen in large MoE language models.

\begin{figure}[h]
\centering
% \includegraphics[width=0.9\linewidth]{fig_moe.pdf}
\caption{Average gate probabilities for the 4 constraint experts.}
\label{fig:moe}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ablation Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Ablation findings are summarized in Table~\ref{tab:ablations}.
Removing key components significantly degrades feasibility and gap.

\begin{table}[h]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Variant} & \textbf{Gap (\%)} & \textbf{Feasible (\%)} \\
\midrule
Ours (full)                     & \textbf{2.8} & \textbf{97} \\
No refinement                   & 7.4 & 68  \\
Single constraint MLP          & 5.9 & 73  \\
No soft relaxation             & 9.2 & 51  \\
\bottomrule
\end{tabular}
\caption{Ablation study results.}
\label{tab:ablations}
\end{table}

Latent refinement and constraint experts contribute most to feasibility
recovery and gap reduction.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Qualitative Routing Behavior}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In qualitative inspections, the surrogate produces:
(i)~globally smooth routing patterns,
(ii)~consistent flow-balanced solutions,
and (iii)~fewer redundant edges than Neural CO baselines.

In high-demand instances, the model naturally routes volume away from
congested hubs, a behavior learned implicitly from the MILP structure.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Summary}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Overall, the Neural Surrogate MILP Solver delivers:
(i) fast and feasible solutions,
(ii) strong scalability, and
(iii) emergent structural understanding of MILP constraints.
These properties make it a promising candidate for real-time 
re-optimization in large supply-chain networks such as Amazon's 
middle-mile routing system.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Future Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This work demonstrates that neural surrogate optimization can capture
the structural behavior of routing MILPs while offering real-time
inference.  
Unlike conventional Neural CO models that operate as learned heuristics,
our surrogate incorporates MILP algebra directly into the forward pass,
producing solutions that respect constraint geometry and exhibit
meaningful feasibility patterns.  
We now discuss why the method works, where it falls short, and how it
may evolve toward large-scale, foundation-style optimization models.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Why Neural Surrogates Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Three factors contribute to the effectiveness of the approach:

\paragraph{(1) MILP-Induced Inductive Bias.}
By embedding $Ax - b$ into the model’s computation and training
objective, the surrogate learns the geometry of the feasible region
instead of memorizing solutions.  
This gives stronger generalization than direct one-shot Neural CO
decoders.

\paragraph{(2) Expert Specialization.}
The MoE layer enables different constraint families---flow, capacity,
timing/SLA, activation---to be handled by dedicated neural experts,
mirroring the specialization of cut families or dual variables in
classical solvers.  
This structured decomposition is rarely explored in Neural CO literature
and is a key reason for improved feasibility.

\paragraph{(3) Differentiable Refinement.}
The latent refinement loop behaves like a fixed-budget approximation to
dual ascent or interior-point steps.  
Instead of searching a combinatorial branch-and-bound tree, the model
performs a small number of smooth corrective updates that can be trained
end-to-end.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Limitations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%}

Despite promising results, several limitations remain:

\begin{itemize}[leftmargin=1.2em,itemsep=2pt]
    \item \textbf{No optimality guarantees.}  
    The method approximates MILPs but does not provide certified bounds.
    
    \item \textbf{Generalization beyond routing.}  
    Performance deteriorates on MILP families that differ dramatically
    from routing/flow problems unless retrained.

    \item \textbf{Constraint expressiveness.}  
    Highly non-linear or disjunctive constraints (e.g., big-$M$ time
    windows with discontinuities) may be difficult for experts to model.

    \item \textbf{Refinement stability.}  
    Very sharp relaxations $\tau\!\to\!0$ can cause unstable training
    dynamics, while softer relaxations reduce discrete fidelity.
\end{itemize}

These limitations suggest that surrogate MILP solvers should be viewed
as complementary to, rather than a replacement for, exact optimization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Future Work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%}

Several directions could significantly extend the capability of neural
MILP surrogates:

\paragraph{Diffusion Models for Discrete Batching and Routing.}
Diffusion models provide powerful generative priors for structured
discrete sets.  
Combining diffusion processes with MILP relaxations could enable
high-quality proposal distributions for batching, clustering, or
multi-route generation under Amazon-style SLAs.

\paragraph{World Models for Multi-Step Logistics Planning.}
Routing decisions propagate congestion and SLA slack through time.
A learned world model predicting future logistics states would allow
long-horizon planning similar to model-based RL or MuZero.  
Coupling a MILP surrogate with a world model may yield an end-to-end
planner capable of optimizing entire daily transportation plans.

\paragraph{Generative MILP Priors and Pretraining.}
Large pre-trained models over diverse MILPs could produce universal
feasibility priors, analogous to foundation models for language.  
Such priors may dramatically improve convergence and generalization.

\paragraph{Hybrid Neural--MILP Pipelines.}
The surrogate can warm-start Gurobi or CPLEX, reducing branch-and-bound
depth and enabling near-optimal solutions under strict time budgets.

\paragraph{Graph-Based Architectures.}
Integrating Graphormer or SSM-based architectures (e.g., Mamba) may
enhance long-range dependency modeling and improve refinement stability
on large networks.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Closing Remarks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Neural surrogate MILP solvers represent a promising convergence of deep
learning and classical combinatorial optimization.  
By embedding constraint algebra, specialization, and refinement into a
single differentiable architecture, these models offer a practical path
toward real-time, high-scale optimization in Amazon’s middle-mile and
global supply-chain systems.  
Future extensions involving diffusion priors, world models, and hybrid
solver pipelines may push this line of work toward general-purpose,
foundation-style optimization models.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We introduced a Neural Surrogate MILP Solver that approximates the
structure and reasoning behavior of classical mixed-integer linear
programs while achieving real-time inference.  
Unlike prior Neural CO models, our approach directly embeds MILP algebra
into the forward pass through soft integer relaxation, constraint
specialization via Mixture-of-Experts, and a differentiable refinement
loop that mimics interior-point or dual-descent steps.  

Experiments on 50--200 node routing benchmarks demonstrate that the
surrogate achieves competitive optimality gaps while offering
$50$--$100\times$ faster inference than commercial MILP solvers.
Moreover, constraint experts exhibit meaningful specialization, and the
refinement process reliably improves feasibility, illustrating that the
model captures nontrivial optimization structure rather than merely
learning heuristics.

Our work highlights a promising direction for large-scale logistics
optimization: learned, amortized surrogates that blend classical
mathematical structure with modern deep learning.  
Future directions include autoregressive multi-route planning, hybrid
neural–MILP pipelines for warm-starting exact solvers, and the
development of foundation models trained across broad families of
combinatorial optimization problems.  

Overall, neural surrogate optimization presents a compelling path toward
real-time, globally consistent decision-making in Amazon-scale
supply-chain systems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliography{references}
\bibliographystyle{icml2026}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\onecolumn
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{appendix_duality}

\section{Appendix}
Add extended experiments, proofs, and extra figures.

\end{document}
